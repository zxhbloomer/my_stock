"""
LightGBMå‚æ•°ä¼˜åŒ–è„šæœ¬
ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ï¼ˆBayesian Optimizationï¼‰è°ƒä¼˜LightGBMå‚æ•°

ç›®æ ‡ï¼š
- æœ€å¤§åŒ–ICå‡å€¼
- æå‡é¢„æµ‹å‡†ç¡®ç‡
- å‡å°‘è¿‡æ‹Ÿåˆé£é™©

ä¼˜åŒ–å‚æ•°ï¼š
- num_leaves: å¶å­èŠ‚ç‚¹æ•°ï¼ˆæ§åˆ¶æ¨¡å‹å¤æ‚åº¦ï¼‰
- learning_rate: å­¦ä¹ ç‡
- feature_fraction: ç‰¹å¾é‡‡æ ·ç‡
- bagging_fraction: æ ·æœ¬é‡‡æ ·ç‡
- max_depth: æ ‘çš„æœ€å¤§æ·±åº¦
- min_data_in_leaf: å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°

ä½œè€…ï¼šClaude Code
æ—¥æœŸï¼š2025-11-15
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, Any, Tuple

# Qlib
import qlib
from qlib.contrib.model.gbdt import LGBModel
from qlib.data.dataset import DatasetH
from qlib.utils import init_instance_by_config
from qlib.workflow import R

# è´å¶æ–¯ä¼˜åŒ–
try:
    from bayes_opt import BayesianOptimization
    BAYES_OPT_AVAILABLE = True
except ImportError:
    print("âš ï¸ bayesian-optimizationæœªå®‰è£…")
    print("   è¯·è¿è¡Œ: pip install bayesian-optimization")
    BAYES_OPT_AVAILABLE = False


class LightGBMOptimizer:
    """LightGBMå‚æ•°ä¼˜åŒ–å™¨"""

    def __init__(
        self,
        dataset_config: Dict[str, Any],
        train_start='2008-01-01',
        train_end='2014-12-31',
        valid_start='2015-01-01',
        valid_end='2016-12-31'
    ):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨

        Args:
            dataset_config: æ•°æ®é›†é…ç½®
            train_start: è®­ç»ƒå¼€å§‹æ—¶é—´
            train_end: è®­ç»ƒç»“æŸæ—¶é—´
            valid_start: éªŒè¯å¼€å§‹æ—¶é—´
            valid_end: éªŒè¯ç»“æŸæ—¶é—´
        """
        self.dataset_config = dataset_config
        self.train_start = train_start
        self.train_end = train_end
        self.valid_start = valid_start
        self.valid_end = valid_end

        # åˆå§‹åŒ–æ•°æ®é›†
        self.dataset = None
        self.best_params = None
        self.best_score = -np.inf
        self.optimization_history = []

    def load_dataset(self) -> None:
        """åŠ è½½æ•°æ®é›†"""
        self.dataset = init_instance_by_config(self.dataset_config)
        print("âœ… æ•°æ®é›†åŠ è½½å®Œæˆ")

    def objective_function(
        self,
        num_leaves,
        learning_rate,
        feature_fraction,
        bagging_fraction,
        max_depth,
        min_data_in_leaf
    ) -> float:
        """
        ç›®æ ‡å‡½æ•°ï¼ˆICå‡å€¼ï¼‰

        Args:
            num_leaves: å¶å­èŠ‚ç‚¹æ•°
            learning_rate: å­¦ä¹ ç‡
            feature_fraction: ç‰¹å¾é‡‡æ ·ç‡
            bagging_fraction: æ ·æœ¬é‡‡æ ·ç‡
            max_depth: æœ€å¤§æ·±åº¦
            min_data_in_leaf: å¶å­æœ€å°æ ·æœ¬æ•°

        Returns:
            float: ICå‡å€¼ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
        """
        # å‚æ•°è½¬æ¢ï¼ˆè´å¶æ–¯ä¼˜åŒ–è¦æ±‚æµ®ç‚¹æ•°ï¼Œéœ€è½¬ä¸ºæ•´æ•°ï¼‰
        num_leaves = int(num_leaves)
        max_depth = int(max_depth)
        min_data_in_leaf = int(min_data_in_leaf)

        # æ„å»ºLightGBMé…ç½®
        model_config = {
            'class': 'LGBModel',
            'module_path': 'qlib.contrib.model.gbdt',
            'kwargs': {
                'loss': 'mse',
                'num_leaves': num_leaves,
                'learning_rate': learning_rate,
                'feature_fraction': feature_fraction,
                'bagging_fraction': bagging_fraction,
                'max_depth': max_depth,
                'min_data_in_leaf': min_data_in_leaf,
                'verbosity': -1
            }
        }

        try:
            # è®­ç»ƒæ¨¡å‹
            model = init_instance_by_config(model_config)
            model.fit(self.dataset)

            # é¢„æµ‹éªŒè¯é›†
            pred = model.predict(self.dataset)

            # è®¡ç®—ICï¼ˆSpearmanç›¸å…³ç³»æ•°ï¼‰
            from scipy.stats import spearmanr

            # è·å–éªŒè¯é›†æ•°æ®
            valid_pred = pred.loc(axis=0)[self.valid_start:self.valid_end]
            valid_label = self.dataset.prepare('valid', col_set='label')

            # å¯¹é½ç´¢å¼•
            common_idx = valid_pred.index.intersection(valid_label.index)
            valid_pred = valid_pred.loc[common_idx]
            valid_label = valid_label.loc[common_idx]

            # è®¡ç®—æ¯æ—¥IC
            ic_list = []
            for date in valid_pred.index.get_level_values('datetime').unique():
                daily_pred = valid_pred.xs(date, level='datetime')
                daily_label = valid_label.xs(date, level='datetime')

                # å¯¹é½
                common_stocks = daily_pred.index.intersection(daily_label.index)
                if len(common_stocks) > 10:
                    pred_values = daily_pred.loc[common_stocks].values.ravel()
                    label_values = daily_label.loc[common_stocks].values.ravel()

                    # å»é™¤NaN
                    mask = ~(np.isnan(pred_values) | np.isnan(label_values))
                    if mask.sum() > 10:
                        ic, _ = spearmanr(pred_values[mask], label_values[mask])
                        ic_list.append(ic)

            # ICå‡å€¼
            ic_mean = np.mean(ic_list) if len(ic_list) > 0 else -1.0

            # è®°å½•å†å²
            self.optimization_history.append({
                'num_leaves': num_leaves,
                'learning_rate': learning_rate,
                'feature_fraction': feature_fraction,
                'bagging_fraction': bagging_fraction,
                'max_depth': max_depth,
                'min_data_in_leaf': min_data_in_leaf,
                'ic_mean': ic_mean
            })

            print(f"   IC={ic_mean:.4f} | leaves={num_leaves}, lr={learning_rate:.4f}, "
                  f"feat_frac={feature_fraction:.2f}, bag_frac={bagging_fraction:.2f}")

            return ic_mean

        except Exception as e:
            print(f"   âŒ è®­ç»ƒå¤±è´¥: {e}")
            return -1.0  # å¤±è´¥è¿”å›æœ€å·®åˆ†æ•°

    def optimize(
        self,
        n_iter=30,
        init_points=5
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œè´å¶æ–¯ä¼˜åŒ–

        Args:
            n_iter: ä¼˜åŒ–è¿­ä»£æ¬¡æ•°
            init_points: åˆå§‹éšæœºæ¢ç´¢ç‚¹æ•°

        Returns:
            dict: æœ€ä¼˜å‚æ•°
        """
        if not BAYES_OPT_AVAILABLE:
            raise ImportError("è¯·å…ˆå®‰è£…bayesian-optimization")

        if self.dataset is None:
            self.load_dataset()

        print("\\n" + "="*80)
        print("å¼€å§‹LightGBMå‚æ•°ä¼˜åŒ–")
        print("="*80)
        print(f"ä¼˜åŒ–ç›®æ ‡: æœ€å¤§åŒ–ICå‡å€¼ï¼ˆéªŒè¯é›†{self.valid_start}è‡³{self.valid_end}ï¼‰")
        print(f"ä¼˜åŒ–æ–¹æ³•: è´å¶æ–¯ä¼˜åŒ–")
        print(f"è¿­ä»£æ¬¡æ•°: {n_iter}")
        print(f"åˆå§‹æ¢ç´¢: {init_points}\\n")

        # å®šä¹‰å‚æ•°ç©ºé—´
        pbounds = {
            'num_leaves': (20, 100),           # å¶å­èŠ‚ç‚¹æ•°
            'learning_rate': (0.01, 0.3),      # å­¦ä¹ ç‡
            'feature_fraction': (0.6, 1.0),    # ç‰¹å¾é‡‡æ ·ç‡
            'bagging_fraction': (0.6, 1.0),    # æ ·æœ¬é‡‡æ ·ç‡
            'max_depth': (3, 10),              # æœ€å¤§æ·±åº¦
            'min_data_in_leaf': (10, 100)      # å¶å­æœ€å°æ ·æœ¬æ•°
        }

        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = BayesianOptimization(
            f=self.objective_function,
            pbounds=pbounds,
            random_state=42,
            verbose=0
        )

        # æ‰§è¡Œä¼˜åŒ–
        optimizer.maximize(
            init_points=init_points,
            n_iter=n_iter
        )

        # æå–æœ€ä¼˜å‚æ•°
        best_params = optimizer.max['params']
        best_params['num_leaves'] = int(best_params['num_leaves'])
        best_params['max_depth'] = int(best_params['max_depth'])
        best_params['min_data_in_leaf'] = int(best_params['min_data_in_leaf'])

        self.best_params = best_params
        self.best_score = optimizer.max['target']

        print("\\n" + "="*80)
        print("ä¼˜åŒ–å®Œæˆï¼")
        print("="*80)
        print(f"\\nğŸ† æœ€ä¼˜ICå‡å€¼: {self.best_score:.4f}")
        print(f"\\nğŸ“‹ æœ€ä¼˜å‚æ•°ï¼š")
        for param, value in best_params.items():
            print(f"   - {param}: {value}")

        return best_params

    def save_results(
        self,
        output_path=None
    ) -> None:
        """
        ä¿å­˜ä¼˜åŒ–ç»“æœ

        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # é»˜è®¤ä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•çš„docsç›®å½•
        if output_path is None:
            project_root = Path(__file__).parent.parent
            output_path = project_root / 'docs' / 'lightgbm_optimization_results.txt'

        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\\n")
            f.write("LightGBMå‚æ•°ä¼˜åŒ–ç»“æœ\\n")
            f.write("="*80 + "\\n\\n")

            f.write(f"ä¼˜åŒ–æ—¶é—´: {datetime.now()}\\n")
            f.write(f"éªŒè¯é›†: {self.valid_start} ~ {self.valid_end}\\n")
            f.write(f"ä¼˜åŒ–è¿­ä»£: {len(self.optimization_history)}\\n\\n")

            f.write(f"ğŸ† æœ€ä¼˜ICå‡å€¼: {self.best_score:.4f}\\n\\n")

            f.write("ğŸ“‹ æœ€ä¼˜å‚æ•°é…ç½®ï¼š\\n")
            f.write("```yaml\\n")
            f.write("model:\\n")
            f.write("  class: LGBModel\\n")
            f.write("  module_path: qlib.contrib.model.gbdt\\n")
            f.write("  kwargs:\\n")
            for param, value in self.best_params.items():
                f.write(f"    {param}: {value}\\n")
            f.write("```\\n\\n")

            # ä¼˜åŒ–å†å²ï¼ˆTop 10ï¼‰
            history_df = pd.DataFrame(self.optimization_history)
            history_df = history_df.sort_values('ic_mean', ascending=False)

            f.write("ğŸ“Š ä¼˜åŒ–å†å²ï¼ˆTop 10ï¼‰ï¼š\\n")
            f.write(history_df.head(10).to_string(index=False))
            f.write("\\n\\n")

            f.write("="*80 + "\\n")

        print(f"\\n[OK] ä¼˜åŒ–ç»“æœå·²ä¿å­˜è‡³: {output_path}")

        # åŒæ—¶ä¿å­˜ä¸ºCSV
        csv_path = output_path.parent / output_path.name.replace('.txt', '.csv')
        history_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"[OK] ä¼˜åŒ–å†å²å·²ä¿å­˜è‡³: {csv_path}")


# å‘½ä»¤è¡Œè¿è¡Œ
if __name__ == "__main__":
    """å‘½ä»¤è¡Œä½¿ç”¨ç¤ºä¾‹"""
    import argparse

    parser = argparse.ArgumentParser(description='LightGBMå‚æ•°ä¼˜åŒ–')
    parser.add_argument(
        '--n-iter',
        type=int,
        default=30,
        help='ä¼˜åŒ–è¿­ä»£æ¬¡æ•°ï¼ˆé»˜è®¤30ï¼‰'
    )
    parser.add_argument(
        '--init-points',
        type=int,
        default=5,
        help='åˆå§‹éšæœºæ¢ç´¢ç‚¹æ•°ï¼ˆé»˜è®¤5ï¼‰'
    )
    parser.add_argument(
        '--instruments',
        type=str,
        default='csi300',
        help='è‚¡ç¥¨æ± ï¼ˆé»˜è®¤csi300ï¼‰'
    )

    args = parser.parse_args()

    # åˆå§‹åŒ–Qlib
    print("åˆå§‹åŒ–Qlib...")
    qlib.init(provider_uri='D:/Data/my_stock', region='cn')

    # æ•°æ®é›†é…ç½®ï¼ˆä½¿ç”¨OptimizedHandlerï¼‰
    dataset_config = {
        'class': 'DatasetH',
        'module_path': 'qlib.data.dataset',
        'kwargs': {
            'handler': {
                'class': 'OptimizedHandler',
                'module_path': 'handlers.optimized_handler',
                'kwargs': {
                    'instruments': args.instruments,
                    'start_time': '2008-01-01',
                    'end_time': '2020-12-31',
                    'fit_start_time': '2008-01-01',
                    'fit_end_time': '2014-12-31',
                    'ic_threshold': 0.01,
                    'use_factor_selector': True
                }
            },
            'segments': {
                'train': ('2008-01-01', '2014-12-31'),
                'valid': ('2015-01-01', '2016-12-31'),
                'test': ('2017-01-01', '2020-12-31')
            }
        }
    }

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = LightGBMOptimizer(
        dataset_config=dataset_config,
        train_start='2008-01-01',
        train_end='2014-12-31',
        valid_start='2015-01-01',
        valid_end='2016-12-31'
    )

    # æ‰§è¡Œä¼˜åŒ–
    best_params = optimizer.optimize(
        n_iter=args.n_iter,
        init_points=args.init_points
    )

    # ä¿å­˜ç»“æœ
    optimizer.save_results()

    print("\\n[OK] å‚æ•°ä¼˜åŒ–å®Œæˆï¼")
    print("\\nä¸‹ä¸€æ­¥ï¼š")
    print("   1. å°†æœ€ä¼˜å‚æ•°æ›´æ–°åˆ° configs/workflow_config_custom.yaml")
    print("   2. è¿è¡Œå®Œæ•´workflowéªŒè¯æ€§èƒ½æå‡")
