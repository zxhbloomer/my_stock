#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Tushareæ•°æ®è½¬QlibäºŒè¿›åˆ¶æ ¼å¼è½¬æ¢å·¥å…·

åŠŸèƒ½:
1. ä»PostgreSQLè¯»å–Tushareæ ¼å¼è‚¡ç¥¨æ•°æ®ï¼ˆ2008-01-01èµ·ï¼‰
2. è½¬æ¢ä¸ºQlibæ ‡å‡†äºŒè¿›åˆ¶æ ¼å¼(.binæ–‡ä»¶)
3. æ™ºèƒ½å¢é‡æ›´æ–°(è‡ªåŠ¨æ£€æµ‹ä¸Šæ¬¡è½¬æ¢æ—¶é—´)
4. å…ƒæ•°æ®è·Ÿè¸ª(.metadata.yaml)
5. è¿›åº¦æ˜¾ç¤ºå’Œå®Œæ•´æ—¥å¿—

ä½œè€…: Claude Code
åˆ›å»ºæ—¶é—´: 2025-11-14
æ•°æ®èŒƒå›´: 2008-01-01 è‡³æœ€æ–°äº¤æ˜“æ—¥
"""

import argparse
import sys
import time
import yaml
import logging
import psycopg2
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from tqdm import tqdm
from typing import Optional, Dict, List


# ==================== é…ç½®ç±» ====================

class Config:
    """é…ç½®ç®¡ç†ç±»"""

    # PostgreSQLé…ç½®
    PG_HOST = "127.0.0.1"
    PG_PORT = 5432
    PG_DATABASE = "my_stock"
    PG_USER = "root"
    PG_PASSWORD = "123456"

    # æ•°æ®èµ·å§‹æ—¥æœŸï¼ˆç”¨æˆ·æŒ‡å®šï¼‰
    START_DATE = "2008-01-01"

    # Qlibæ•°æ®è·¯å¾„
    QLIB_DIR = Path(r"D:\Data\my_stock")

    # ä¸´æ—¶ç›®å½•
    TEMP_DIR = Path(__file__).parent.parent / "temp_qlib_data"

    # å…ƒæ•°æ®æ–‡ä»¶
    METADATA_FILE = ".metadata.yaml"

    # æ—¥å¿—é…ç½®
    LOG_FORMAT = '%(asctime)s - %(levelname)s - %(message)s'
    LOG_LEVEL = logging.INFO


# ==================== æ—¥å¿—é…ç½® ====================

def setup_logging(verbose: bool = False):
    """é…ç½®æ—¥å¿—"""
    level = logging.DEBUG if verbose else Config.LOG_LEVEL

    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    Config.QLIB_DIR.mkdir(parents=True, exist_ok=True)

    logging.basicConfig(
        level=level,
        format=Config.LOG_FORMAT,
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(
                Config.QLIB_DIR / "tushare_to_qlib.log",
                mode='a',
                encoding='utf-8'
            )
        ]
    )
    return logging.getLogger(__name__)


# ==================== æ•°æ®åº“æ“ä½œ ====================

class DatabaseManager:
    """PostgreSQLæ•°æ®åº“ç®¡ç†å™¨"""

    def __init__(self, logger):
        self.logger = logger
        self.conn = None

    def connect(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.conn = psycopg2.connect(
                host=Config.PG_HOST,
                port=Config.PG_PORT,
                database=Config.PG_DATABASE,
                user=Config.PG_USER,
                password=Config.PG_PASSWORD
            )
            self.logger.info(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ: {Config.PG_HOST}:{Config.PG_PORT}/{Config.PG_DATABASE}")
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()
            self.logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")

    def get_latest_trade_date(self) -> Optional[str]:
        """è·å–æœ€æ–°äº¤æ˜“æ—¥æœŸ"""
        sql = f"""
        SELECT MAX(trade_date)
        FROM tushare.stock_daily
        WHERE trade_date >= '{Config.START_DATE}'
        """
        with self.conn.cursor() as cur:
            cur.execute(sql)
            result = cur.fetchone()
            return str(result[0]) if result and result[0] else None

    def get_listed_stocks(self) -> pd.DataFrame:
        """è·å–ä¸Šå¸‚è‚¡ç¥¨åˆ—è¡¨(æ’é™¤é€€å¸‚è‚¡ç¥¨)"""
        sql = """
        SELECT
            ts_code,
            list_date,
            COALESCE(delist_date, '2099-12-31') as delist_date
        FROM tushare.stock_basic
        WHERE list_status = 'L'
        ORDER BY ts_code
        """
        return pd.read_sql(sql, self.conn)

    def export_daily_data(self, last_date: Optional[str] = None) -> pd.DataFrame:
        """
        å¯¼å‡ºæ—¥çº¿æ•°æ®ï¼ˆä»2008-01-01å¼€å§‹ï¼‰

        Args:
            last_date: ä¸Šæ¬¡æ›´æ–°æ—¥æœŸ,å¦‚æœä¸ºNoneåˆ™å¯¼å‡ºå…¨éƒ¨æ•°æ®
        """
        # å¢é‡æ¨¡å¼ï¼šlast_dateä¹‹åçš„æ•°æ®
        # å…¨é‡æ¨¡å¼ï¼š2008-01-01ä¹‹åçš„æ‰€æœ‰æ•°æ®
        if last_date:
            date_filter = f"AND d.trade_date > '{last_date}'"
        else:
            date_filter = ""

        sql = f"""
        SELECT
            d.ts_code,
            d.trade_date as date,
            d.open,
            d.high,
            d.low,
            d.close,
            d.volume,
            d.amount,
            f.adj_factor as factor
        FROM tushare.stock_daily d
        INNER JOIN tushare.stock_basic b ON d.ts_code = b.ts_code
        LEFT JOIN tushare.stock_adjfactor f
            ON d.ts_code = f.ts_code AND d.trade_date = f.trade_date
        WHERE b.list_status = 'L'
          AND d.trade_date >= '{Config.START_DATE}'
          {date_filter}
        ORDER BY d.ts_code, d.trade_date
        """

        mode_desc = f"å¢é‡: > {last_date}" if last_date else f"å…¨é‡: >= {Config.START_DATE}"
        self.logger.info(f"å¼€å§‹å¯¼å‡ºè‚¡ç¥¨æ•°æ®... ({mode_desc})")

        df = pd.read_sql(sql, self.conn)
        self.logger.info(f"âœ… è‚¡ç¥¨æ•°æ®å¯¼å‡ºå®Œæˆ: {len(df):,} æ¡è®°å½•")

        return df

    def export_index_daily_data(self, last_date: Optional[str] = None) -> pd.DataFrame:
        """
        å¯¼å‡ºæŒ‡æ•°æ—¥çº¿æ•°æ®ï¼ˆä»2008-01-01å¼€å§‹ï¼‰

        Args:
            last_date: ä¸Šæ¬¡æ›´æ–°æ—¥æœŸ,å¦‚æœä¸ºNoneåˆ™å¯¼å‡ºå…¨éƒ¨æ•°æ®
        """
        if last_date:
            date_filter = f"AND trade_date > '{last_date}'"
        else:
            date_filter = ""

        sql = f"""
        SELECT
            ts_code,
            trade_date as date,
            open,
            high,
            low,
            close,
            volume,
            amount,
            1.0 as factor
        FROM tushare.index_daily
        WHERE trade_date >= '{Config.START_DATE}'
          {date_filter}
        ORDER BY ts_code, trade_date
        """

        mode_desc = f"å¢é‡: > {last_date}" if last_date else f"å…¨é‡: >= {Config.START_DATE}"
        self.logger.info(f"å¼€å§‹å¯¼å‡ºæŒ‡æ•°æ•°æ®... ({mode_desc})")

        df = pd.read_sql(sql, self.conn)
        self.logger.info(f"âœ… æŒ‡æ•°æ•°æ®å¯¼å‡ºå®Œæˆ: {len(df):,} æ¡è®°å½•")

        return df

    def get_trading_calendar(self, last_date: Optional[str] = None) -> List[str]:
        """
        è·å–äº¤æ˜“æ—¥å†ï¼ˆä»2008-01-01å¼€å§‹ï¼‰

        Args:
            last_date: ä¸Šæ¬¡æ›´æ–°æ—¥æœŸ,å¦‚æœä¸ºNoneåˆ™è·å–å…¨éƒ¨äº¤æ˜“æ—¥
        """
        if last_date:
            date_filter = f"AND cal_date > '{last_date}'"
        else:
            date_filter = ""

        sql = f"""
        SELECT DISTINCT cal_date
        FROM tushare.others_calendar
        WHERE is_open = 1
          AND exchange = 'SSE'
          AND cal_date >= '{Config.START_DATE}'
          {date_filter}
        ORDER BY cal_date
        """

        with self.conn.cursor() as cur:
            cur.execute(sql)
            return [str(row[0]) for row in cur.fetchall()]

    def get_index_constituents(self, index_code: str) -> tuple:
        """
        è·å–æŒ‡æ•°æˆåˆ†è‚¡åˆ—è¡¨ï¼ˆæœ€æ–°æ—¥æœŸï¼‰

        Args:
            index_code: æŒ‡æ•°ä»£ç ï¼ˆå¦‚ 000300.SHï¼‰

        Returns:
            (è‚¡ç¥¨ä»£ç åˆ—è¡¨, æˆåˆ†è‚¡æ—¥æœŸ)
        """
        # è·å–æœ€æ–°æ—¥æœŸ
        sql_date = f"""
        SELECT MAX(trade_date)
        FROM tushare.index_weight
        WHERE index_code = '{index_code}'
        """

        with self.conn.cursor() as cur:
            cur.execute(sql_date)
            latest_date = cur.fetchone()[0]

        if not latest_date:
            return [], None

        # è·å–æˆåˆ†è‚¡
        sql = f"""
        SELECT DISTINCT con_code
        FROM tushare.index_weight
        WHERE index_code = '{index_code}'
          AND trade_date = '{latest_date}'
        ORDER BY con_code
        """

        with self.conn.cursor() as cur:
            cur.execute(sql)
            stocks = [row[0] for row in cur.fetchall()]

        return stocks, str(latest_date)


# ==================== è‚¡ç¥¨ä»£ç è½¬æ¢ ====================

def convert_ts_code_to_qlib(ts_code: str) -> str:
    """
    è½¬æ¢Tushareä»£ç ä¸ºQlibæ ¼å¼ï¼ˆæ”¯æŒè‚¡ç¥¨å’ŒæŒ‡æ•°ï¼‰

    Args:
        ts_code: Tushareæ ¼å¼ä»£ç  (å¦‚ 000001.SZ, 600000.SH, 000300.SH)

    Returns:
        Qlibæ ¼å¼ä»£ç  (å¦‚ SZ000001, SH600000, SH000300)

    Examples:
        >>> convert_ts_code_to_qlib('000001.SZ')
        'SZ000001'
        >>> convert_ts_code_to_qlib('600000.SH')
        'SH600000'
        >>> convert_ts_code_to_qlib('000300.SH')
        'SH000300'
    """
    if '.' not in ts_code:
        return ts_code

    symbol, exchange = ts_code.split('.')
    return f"{exchange}{symbol}"


# ==================== å…ƒæ•°æ®ç®¡ç† ====================

class MetadataManager:
    """å…ƒæ•°æ®ç®¡ç†å™¨"""

    def __init__(self, qlib_dir: Path, logger):
        self.qlib_dir = qlib_dir
        self.metadata_file = qlib_dir / Config.METADATA_FILE
        self.logger = logger

    def read(self) -> Optional[Dict]:
        """è¯»å–å…ƒæ•°æ®"""
        if not self.metadata_file.exists():
            return None

        try:
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            self.logger.warning(f"è¯»å–å…ƒæ•°æ®å¤±è´¥: {e}")
            return None

    def write(self, metadata: Dict):
        """å†™å…¥å…ƒæ•°æ®"""
        try:
            self.qlib_dir.mkdir(parents=True, exist_ok=True)
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                yaml.dump(metadata, f, allow_unicode=True, default_flow_style=False)
            self.logger.info(f"âœ… å…ƒæ•°æ®å·²ä¿å­˜: {self.metadata_file}")
        except Exception as e:
            self.logger.error(f"âŒ å…ƒæ•°æ®ä¿å­˜å¤±è´¥: {e}")

    def determine_mode(self, force_rebuild: bool, latest_db_date: str) -> str:
        """
        åˆ¤æ–­è½¬æ¢æ¨¡å¼

        Returns:
            'full': å…¨é‡è½¬æ¢
            'incremental': å¢é‡è½¬æ¢
            'skip': æ— éœ€è½¬æ¢
        """
        if force_rebuild:
            self.logger.info("ğŸ”„ å¼ºåˆ¶é‡å»ºæ¨¡å¼")
            return 'full'

        metadata = self.read()
        if not metadata:
            self.logger.info("ğŸ†• é¦–æ¬¡è½¬æ¢,æ‰§è¡Œå…¨é‡è½¬æ¢")
            return 'full'

        last_update_date = metadata.get('last_update_date')
        if not last_update_date:
            self.logger.warning("å…ƒæ•°æ®ç¼ºå°‘last_update_date,æ‰§è¡Œå…¨é‡è½¬æ¢")
            return 'full'

        if latest_db_date <= last_update_date:
            self.logger.info(f"âœ… æ•°æ®å·²æ˜¯æœ€æ–°(DB: {latest_db_date}, ä¸Šæ¬¡: {last_update_date})")
            return 'skip'

        self.logger.info(f"ğŸ“ˆ æ£€æµ‹åˆ°æ–°æ•°æ®: {last_update_date} â†’ {latest_db_date}")
        return 'incremental'


# ==================== æ•°æ®è½¬æ¢ ====================

class DataConverter:
    """æ•°æ®è½¬æ¢å™¨ - ç›´æ¥å†™å…¥QlibäºŒè¿›åˆ¶æ ¼å¼"""

    def __init__(self, logger, qlib_dir: Path):
        self.logger = logger
        self.qlib_dir = qlib_dir
        self.calendars_dir = qlib_dir / "calendars"
        self.instruments_dir = qlib_dir / "instruments"
        self.features_dir = qlib_dir / "features"

    def prepare_directories(self):
        """å‡†å¤‡Qlibç›®å½•ç»“æ„ - æ¯æ¬¡å…¨æ–°åˆ›å»º"""
        import shutil

        # åˆ é™¤æ—§çš„æ•°æ®ç›®å½•
        if self.features_dir.exists():
            self.logger.info(f"ğŸ—‘ï¸ åˆ é™¤æ—§çš„featuresç›®å½•...")
            shutil.rmtree(self.features_dir)

        if self.calendars_dir.exists():
            self.logger.info(f"ğŸ—‘ï¸ åˆ é™¤æ—§çš„calendarsç›®å½•...")
            shutil.rmtree(self.calendars_dir)

        if self.instruments_dir.exists():
            self.logger.info(f"ğŸ—‘ï¸ åˆ é™¤æ—§çš„instrumentsç›®å½•...")
            shutil.rmtree(self.instruments_dir)

        # é‡æ–°åˆ›å»ºç›®å½•
        self.qlib_dir.mkdir(parents=True, exist_ok=True)
        self.calendars_dir.mkdir(parents=True, exist_ok=True)
        self.instruments_dir.mkdir(parents=True, exist_ok=True)
        self.features_dir.mkdir(parents=True, exist_ok=True)
        self.logger.info(f"âœ… Qlibç›®å½•ç»“æ„å·²åˆ›å»º: {self.qlib_dir}")

    def save_calendar(self, dates: List[str]):
        """ä¿å­˜äº¤æ˜“æ—¥å†"""
        calendar_file = self.calendars_dir / "day.txt"
        with open(calendar_file, 'w') as f:
            f.write('\n'.join(dates))
        self.logger.info(f"âœ… äº¤æ˜“æ—¥å†å·²ä¿å­˜: {len(dates)} ä¸ªäº¤æ˜“æ—¥")

    def save_instruments(self, df: pd.DataFrame):
        """
        ä¿å­˜è‚¡ç¥¨åˆ—è¡¨

        Args:
            df: åŒ…å«symbol, earliest_date, latest_dateçš„DataFrame
        """
        instruments_file = self.instruments_dir / "all.txt"

        lines = []
        for _, row in df.iterrows():
            # æ ¼å¼: symbol\tstart_date\tend_date
            lines.append(f"{row['symbol']}\t{row['earliest_date']}\t{row['latest_date']}")

        with open(instruments_file, 'w') as f:
            f.write('\n'.join(lines))

        self.logger.info(f"âœ… è‚¡ç¥¨åˆ—è¡¨å·²ä¿å­˜: {len(lines)} åªè‚¡ç¥¨")

    def save_market_files(self, db: 'DatabaseManager'):
        """
        ä¿å­˜å¸‚åœºæ–‡ä»¶ï¼ˆæŒ‡æ•°æˆåˆ†è‚¡ï¼‰- æ¯æ¬¡å…¨é‡é‡æ–°ç”Ÿæˆ
        æ ¼å¼ï¼šsymbol\tstart_date\tend_dateï¼ˆä¸all.txtä¿æŒä¸€è‡´ï¼‰

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
        """
        # æŒ‡æ•°æ˜ å°„ï¼šæ–‡ä»¶å -> æŒ‡æ•°ä»£ç 
        index_mapping = {
            'csi300': '000300.SH',  # æ²ªæ·±300
            'csi500': '000905.SH',  # ä¸­è¯500
        }

        self.logger.info("ç”Ÿæˆå¸‚åœºæ–‡ä»¶...")

        # è¯»å–all.txtè·å–æ¯åªè‚¡ç¥¨çš„èµ·æ­¢æ—¥æœŸ
        all_file = self.instruments_dir / "all.txt"
        stock_dates = {}
        if all_file.exists():
            with open(all_file, 'r') as f:
                for line in f:
                    parts = line.strip().split('\t')
                    if len(parts) == 3:
                        stock_dates[parts[0]] = (parts[1], parts[2])

        for market_name, index_code in index_mapping.items():
            # è·å–æˆåˆ†è‚¡
            stocks, constituent_date = db.get_index_constituents(index_code)

            if not stocks:
                self.logger.warning(f"  âš ï¸ {market_name} ({index_code}) æ— æˆåˆ†è‚¡æ•°æ®")
                continue

            # è½¬æ¢ä¸ºQlibæ ¼å¼å¹¶è·å–æ—¥æœŸèŒƒå›´
            lines = []
            for ts_code in stocks:
                qlib_code = convert_ts_code_to_qlib(ts_code)

                # ä»all.txtè·å–è¯¥è‚¡ç¥¨çš„èµ·æ­¢æ—¥æœŸ
                if qlib_code in stock_dates:
                    start_date, end_date = stock_dates[qlib_code]
                    lines.append(f"{qlib_code}\t{start_date}\t{end_date}")
                else:
                    # å¦‚æœåœ¨all.txtä¸­æ‰¾ä¸åˆ°ï¼ˆç†è®ºä¸Šä¸åº”è¯¥ï¼‰ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    self.logger.warning(f"  âš ï¸ {qlib_code} ä¸åœ¨all.txtä¸­ï¼Œä½¿ç”¨é»˜è®¤æ—¥æœŸèŒƒå›´")
                    lines.append(f"{qlib_code}\t{Config.START_DATE}\t2099-12-31")

            # å†™å…¥æ–‡ä»¶ï¼ˆ3åˆ—æ ¼å¼ï¼šsymbol\tstart_date\tend_dateï¼‰
            market_file = self.instruments_dir / f"{market_name}.txt"
            with open(market_file, 'w') as f:
                f.write('\n'.join(lines))

            self.logger.info(f"  âœ… {market_name}.txt: {len(lines)}åªè‚¡ç¥¨ (æˆåˆ†è‚¡æ—¥æœŸ: {constituent_date})")

    def write_bin_file(self, symbol: str, field: str, dates_index: Dict[str, int],
                       data: pd.Series, all_dates: List[str]):
        """
        å†™å…¥å•ä¸ªå­—æ®µçš„äºŒè¿›åˆ¶æ–‡ä»¶

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            field: å­—æ®µå
            dates_index: æ—¥æœŸåˆ°ç´¢å¼•çš„æ˜ å°„
            data: æ•°æ®Series (index=date, value=æ•°å€¼)
            all_dates: æ‰€æœ‰äº¤æ˜“æ—¥åˆ—è¡¨
        """
        # åˆ›å»ºè‚¡ç¥¨ç›®å½•
        stock_dir = self.features_dir / symbol.lower()
        stock_dir.mkdir(parents=True, exist_ok=True)

        # äºŒè¿›åˆ¶æ–‡ä»¶è·¯å¾„
        bin_file = stock_dir / f"{field.lower()}.day.bin"

        # å‡†å¤‡æ•°æ®æ•°ç»„
        n_dates = len(all_dates)
        arr = np.full(n_dates, np.nan, dtype=np.float32)

        # å¡«å……æ•°æ®
        for date_str, value in data.items():
            if date_str in dates_index:
                idx = dates_index[date_str]
                arr[idx] = value

        # å†™å…¥äºŒè¿›åˆ¶æ–‡ä»¶ï¼ˆQlibæ ¼å¼ï¼šç¬¬ä¸€ä¸ªfloatæ˜¯èµ·å§‹ç´¢å¼•ï¼‰
        if not np.all(np.isnan(arr)):
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªéNaNçš„ä½ç½®
            valid_indices = np.where(~np.isnan(arr))[0]
            if len(valid_indices) > 0:
                start_idx = valid_indices[0]
                end_idx = valid_indices[-1] + 1

                # å†™å…¥æ ¼å¼ï¼š[start_index, data...]
                with open(bin_file, 'wb') as f:
                    np.array([start_idx], dtype=np.float32).tofile(f)
                    arr[start_idx:end_idx].astype(np.float32).tofile(f)

    def convert_to_qlib(self, df: pd.DataFrame, all_dates: List[str]):
        """
        è½¬æ¢ä¸ºQlibäºŒè¿›åˆ¶æ ¼å¼

        Args:
            df: å®Œæ•´æ•°æ®DataFrame
            all_dates: æ‰€æœ‰äº¤æ˜“æ—¥åˆ—è¡¨
        """
        if df.empty:
            self.logger.warning("âš ï¸ æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡è½¬æ¢")
            return

        # è½¬æ¢è‚¡ç¥¨ä»£ç 
        df['symbol'] = df['ts_code'].apply(convert_ts_code_to_qlib)
        df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')

        # åˆ›å»ºæ—¥æœŸç´¢å¼•æ˜ å°„
        dates_index = {date: idx for idx, date in enumerate(all_dates)}

        # å­—æ®µåˆ—è¡¨
        fields = ['open', 'high', 'low', 'close', 'volume', 'amount', 'factor']

        # æŒ‰è‚¡ç¥¨åˆ†ç»„å¤„ç†
        stock_groups = df.groupby('symbol')
        stock_count = len(stock_groups)

        self.logger.info(f"å¼€å§‹è½¬æ¢ä¸ºQlibäºŒè¿›åˆ¶æ ¼å¼...")

        with tqdm(total=stock_count, desc="è½¬æ¢è‚¡ç¥¨æ•°æ®") as pbar:
            for symbol, group in stock_groups:
                # ä¸ºæ¯ä¸ªå­—æ®µå†™å…¥äºŒè¿›åˆ¶æ–‡ä»¶
                for field in fields:
                    if field in group.columns:
                        data = group.set_index('date')[field]
                        self.write_bin_file(symbol, field, dates_index, data, all_dates)

                pbar.update(1)

        self.logger.info(f"âœ… Qlibè½¬æ¢å®Œæˆ: {stock_count} åªè‚¡ç¥¨")

        # ä¿å­˜è‚¡ç¥¨åˆ—è¡¨
        instruments_df = df.groupby('symbol')['date'].agg(['min', 'max']).reset_index()
        instruments_df.columns = ['symbol', 'earliest_date', 'latest_date']
        self.save_instruments(instruments_df)


# ==================== ä¸»æµç¨‹ ====================

def main():
    """ä¸»å‡½æ•°"""

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='Tushareæ•°æ®è½¬Qlibæ ¼å¼è½¬æ¢å·¥å…·ï¼ˆæ¯æ¬¡å…¨é‡é‡å»ºï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # é»˜è®¤å…¨é‡è½¬æ¢
  python 00_tushare_to_qlib.py

  # æŒ‡å®šè¾“å‡ºè·¯å¾„
  python 00_tushare_to_qlib.py --output D:\\Data\\my_stock

  # æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
  python 00_tushare_to_qlib.py --verbose
        """
    )

    parser.add_argument(
        '--output',
        type=str,
        default=str(Config.QLIB_DIR),
        help=f'è¾“å‡ºç›®å½• (é»˜è®¤: {Config.QLIB_DIR})'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—'
    )

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(args.verbose)

    # å¼€å§‹è½¬æ¢
    start_time = time.time()
    logger.info("=" * 70)
    logger.info("Tushare â†’ Qlib æ•°æ®è½¬æ¢å·¥å…·ï¼ˆå…¨é‡é‡å»ºæ¨¡å¼ï¼‰")
    logger.info(f"æ•°æ®èŒƒå›´: {Config.START_DATE} è‡³æœ€æ–°äº¤æ˜“æ—¥")
    logger.info(f"è¾“å‡ºè·¯å¾„: {args.output}")
    logger.info("=" * 70)

    qlib_dir = Path(args.output)
    db = None
    converter = None

    try:
        # 1. è¿æ¥æ•°æ®åº“
        db = DatabaseManager(logger)
        db.connect()

        # 2. è·å–æœ€æ–°äº¤æ˜“æ—¥æœŸ
        latest_db_date = db.get_latest_trade_date()
        logger.info(f"ğŸ“… æ•°æ®åº“æœ€æ–°äº¤æ˜“æ—¥: {latest_db_date}")

        # 3. å‡†å¤‡è½¬æ¢ï¼ˆæ¯æ¬¡å…¨é‡é‡å»ºï¼‰
        converter = DataConverter(logger, qlib_dir)
        converter.prepare_directories()

        # 4. è·å–äº¤æ˜“æ—¥å†
        logger.info("è·å–äº¤æ˜“æ—¥å†...")
        all_dates = db.get_trading_calendar(last_date=None)  # å…¨éƒ¨äº¤æ˜“æ—¥
        converter.save_calendar(all_dates)

        # 5. å¯¼å‡ºè‚¡ç¥¨æ•°æ®ï¼ˆå…¨é‡ï¼‰
        logger.info(f"å¼€å§‹å¯¼å‡ºè‚¡ç¥¨æ•°æ®... (å…¨é‡: >= {Config.START_DATE})")
        df_stock = db.export_daily_data(last_date=None)

        # 6. å¯¼å‡ºæŒ‡æ•°æ•°æ®ï¼ˆå…¨é‡ï¼‰
        logger.info(f"å¼€å§‹å¯¼å‡ºæŒ‡æ•°æ•°æ®... (å…¨é‡: >= {Config.START_DATE})")
        df_index = db.export_index_daily_data(last_date=None)

        # 7. åˆå¹¶è‚¡ç¥¨å’ŒæŒ‡æ•°æ•°æ®
        df = pd.concat([df_stock, df_index], ignore_index=True)

        if df.empty:
            logger.warning("âš ï¸ æ— æ•°æ®å¯è½¬æ¢")
            return

        logger.info(f"âœ… åˆå¹¶æ•°æ®: è‚¡ç¥¨{len(df_stock):,}æ¡ + æŒ‡æ•°{len(df_index):,}æ¡ = æ€»è®¡{len(df):,}æ¡")

        # 8. è½¬æ¢ä¸ºQlibæ ¼å¼ï¼ˆä¼šç”Ÿæˆall.txtï¼‰
        converter.convert_to_qlib(df, all_dates)

        # 9. ç”Ÿæˆå¸‚åœºæ–‡ä»¶ï¼ˆä¾èµ–all.txtï¼Œå¿…é¡»åœ¨convert_to_qlibä¹‹åï¼‰
        converter.save_market_files(db)

        # 10. æ›´æ–°å…ƒæ•°æ®
        metadata_mgr = MetadataManager(qlib_dir, logger)
        metadata = {
            'version': '1.0',
            'start_date': Config.START_DATE,
            'last_update_date': latest_db_date,
            'last_update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'conversion_mode': 'full',
            'total_stocks': df_stock['ts_code'].nunique() if not df_stock.empty else 0,
            'total_indices': df_index['ts_code'].nunique() if not df_index.empty else 0,
            'total_instruments': df['ts_code'].nunique(),
            'total_records': len(df),
            'earliest_date': df['date'].min(),
            'latest_date': df['date'].max(),
        }
        metadata_mgr.write(metadata)

        # 11. å®Œæˆ
        elapsed = time.time() - start_time
        logger.info("=" * 70)
        logger.info(f"âœ… è½¬æ¢å®Œæˆ!")
        logger.info(f"ğŸ“Š ç»Ÿè®¡:")
        logger.info(f"  - è‚¡ç¥¨æ•°é‡: {metadata['total_stocks']:,}")
        logger.info(f"  - æŒ‡æ•°æ•°é‡: {metadata['total_indices']:,}")
        logger.info(f"  - æ€»å·¥å…·æ•°: {metadata['total_instruments']:,}")
        logger.info(f"  - è®°å½•æ•°é‡: {metadata['total_records']:,}")
        logger.info(f"  - æ—¥æœŸèŒƒå›´: {metadata['earliest_date']} ~ {metadata['latest_date']}")
        logger.info(f"  - è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
        logger.info("=" * 70)

    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ è½¬æ¢å¤±è´¥: {e}", exc_info=True)
        sys.exit(1)
    finally:
        # æ¸…ç†èµ„æº
        if db:
            db.close()


if __name__ == '__main__':
    main()
