"""
Tushare Token è‡ªåŠ¨é‡è½½æ¨¡å—

åŠŸèƒ½ï¼š
- åå°çº¿ç¨‹å®šæœŸæ£€æŸ¥.envæ–‡ä»¶ä¸­çš„TUSHARE_TOKEN
- æ£€æµ‹åˆ°tokenå˜åŒ–æ—¶è‡ªåŠ¨è°ƒç”¨ TushareAPI.update_all_tokens()
- æ”¯æŒé•¿æ—¶é—´è¿è¡Œä»»åŠ¡ï¼ˆå¦‚10å¤©ä»»åŠ¡ï¼Œtokenæ¯3å¤©æ›´æ–°ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    from data.collectors.sources.tushare.token_auto_reload import start_token_auto_reload

    # åœ¨ç¨‹åºå¯åŠ¨æ—¶è°ƒç”¨ä¸€æ¬¡å³å¯
    start_token_auto_reload()  # é»˜è®¤æ¯1å°æ—¶æ£€æŸ¥ä¸€æ¬¡
"""

import logging
import threading
import time
from pathlib import Path
from typing import Optional

from dotenv import dotenv_values


logger = logging.getLogger(__name__)


def _token_reload_loop(env_path: str, check_interval: int):
    """åå°å¾ªç¯ï¼šå®šæœŸæ£€æŸ¥å¹¶é‡è½½token"""
    env_file = Path(env_path).resolve()
    last_token: Optional[str] = None

    logger.info(f"ğŸ” Tokenè‡ªåŠ¨æ£€æŸ¥å·²å¯åŠ¨ (é—´éš”: {check_interval}ç§’)")

    while True:
        try:
            # è¯»å–.envæ–‡ä»¶
            if not env_file.exists():
                logger.warning(f"âŒ .envæ–‡ä»¶ä¸å­˜åœ¨: {env_file}")
                time.sleep(check_interval)
                continue

            env_values = dotenv_values(str(env_file))
            new_token = env_values.get("TUSHARE_TOKEN")

            if not new_token:
                logger.warning("âš ï¸ TUSHARE_TOKENæœªåœ¨.envæ–‡ä»¶ä¸­æ‰¾åˆ°")
                time.sleep(check_interval)
                continue

            # æ£€æŸ¥tokenæ˜¯å¦å˜åŒ–
            if new_token != last_token:
                if last_token is None:
                    logger.info(f"ğŸ“‹ åˆå§‹Token: {new_token[:10]}...{new_token[-6:]}")
                else:
                    logger.info(f"ğŸ”„ Tokenå·²å˜åŒ–ï¼Œæ­£åœ¨æ›´æ–°...")
                    logger.info(f"   æ—§: {last_token}")
                    logger.info(f"   æ–°: {new_token}")

                    # æ›´æ–°ç¯å¢ƒå˜é‡ï¼ŒTushareAPIä¼šåŠ¨æ€è¯»å–
                    try:
                        import os
                        os.environ["TUSHARE_TOKEN"] = new_token
                        logger.info(f"âœ… Tokenå·²æ›´æ–°åˆ°ç¯å¢ƒå˜é‡")
                    except Exception as e:
                        logger.error(f"âŒ æ›´æ–°Tokenå¤±è´¥: {e}")

                last_token = new_token

        except Exception as e:
            logger.error(f"âŒ Tokenæ£€æŸ¥å‡ºé”™: {e}")

        time.sleep(check_interval)


_reload_thread: Optional[threading.Thread] = None


def start_token_auto_reload(env_path: str = ".env", check_interval: int = 3600):
    """
    å¯åŠ¨Tokenè‡ªåŠ¨é‡è½½

    Args:
        env_path: .envæ–‡ä»¶è·¯å¾„
        check_interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤3600ç§’ï¼ˆ1å°æ—¶ï¼‰

    ç¤ºä¾‹:
        # æ¯1å°æ—¶æ£€æŸ¥ä¸€æ¬¡ï¼ˆç”Ÿäº§ç¯å¢ƒæ¨èï¼‰
        start_token_auto_reload()

        # æ¯1åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        start_token_auto_reload(check_interval=60)
    """
    global _reload_thread

    if _reload_thread is not None and _reload_thread.is_alive():
        logger.warning("Tokenè‡ªåŠ¨é‡è½½å·²ç»åœ¨è¿è¡Œ")
        return

    _reload_thread = threading.Thread(
        target=_token_reload_loop,
        args=(env_path, check_interval),
        daemon=True,
        name="TokenAutoReloader"
    )
    _reload_thread.start()

    logger.info(f"âœ… Tokenè‡ªåŠ¨é‡è½½çº¿ç¨‹å·²å¯åŠ¨")
